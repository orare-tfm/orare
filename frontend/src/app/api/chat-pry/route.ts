/**
 * @file  src/app/api/chat-pry/route.ts
 * @description
 * This module handles POST requests from User messages for the chat API.
 * It receives user messages, performs a comparison in Pinecone with bible previously
 * vectorized, select the best result and create a generative response using the GPT-4 model.
 * @date 08/08/2024
 * @maintainer Orare Team
 * @inputs
 * - messages: An array of objects containing user messages and take the last one (messages-1).
 *   Each message must have at least a 'content' field.
 * @outputs
 * - Returns a JSON response with the message generated by GPT-4.
 * @dependencies
 * - openai-edge for integration with the OpenAI API.
 * - comparePrayInPinecone for compare vectors
 */
// src/app/api/chat-pry/route.ts
// src/app/api/chat-pry/route.ts
// src/app/api/chat-pry/route.ts

import { Configuration, OpenAIApi } from "openai-edge";
import { NextResponse } from "next/server";
import { Message, OpenAIStream, StreamingTextResponse } from "ai";
import { comparePrayInPinecone } from "@/lib/pinecone";
import { generatePromptSystem, generatePromptAssistant } from "@/lib/prompts";

// Connection to OpenAI API
const config = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(config);

// Handling POST requests from user messages
export async function POST(req: Request) {
  try {
    const { messages } = await req.json();
    if (!messages || messages.length === 0) {
      throw new Error("No se proporcionaron mensajes.");
    }

    const userMessage = messages[messages.length - 1].content;

    const pineconeResults = await comparePrayInPinecone(userMessage);
    if (pineconeResults.length === 0) {
      throw new Error("No se encontraron resultados en Pinecone.");
    }


    const highestScoreObject = pineconeResults.reduce((max, obj) => {
      const currentScore = obj.score ?? -Infinity; // Ensure obj.score doesn't be undefined
      const maxScore = max.score ?? -Infinity; // Ensure max.score doesn't be undefined
      return currentScore > maxScore ? obj : max;
    }, pineconeResults[0]);
    
    console.log('highestScoreObject', highestScoreObject);
    console.log('pineconeResults', pineconeResults)


    const promptSystem = generatePromptSystem(userMessage, pineconeResults);
    const promtAssistant = generatePromptAssistant();

    const response = await openai.createChatCompletion({
      model: "gpt-4o",
      stream: true,
      temperature: 1.0,
      top_p: 0.6,
      messages: [
        { role: "system", content: promptSystem },
        { role: "assistant", content: promtAssistant },
        ...messages.filter((message: Message) => message.role === "user"),
      ],
    });

    const stream = OpenAIStream(response);
    return new StreamingTextResponse(stream);
  } catch (error) {
    console.error("Error en la solicitud POST:", error);
    return NextResponse.json({
      message: "Error al procesar la solicitud POST.",
    });
  }
}
